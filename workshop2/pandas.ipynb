{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e27f3f",
   "metadata": {},
   "source": [
    "---\n",
    "# Pandas\n",
    "---\n",
    "\n",
    "How to use this notebook:\n",
    "\n",
    "- First, work through the prerequisites in [1. Prerequisites](#1-prerequisites).\n",
    "- Then, work though [2. The Pandas Series Object](#2-the-pandas-series-object) to [15. Saving and Loading Series and DataFrames](#15-saving-and-loading-series-and-dataframes).\n",
    "  - Note that you don't have to work through all chapters at once.\n",
    "  - The various chapters are also meant as a reference if you need to re-visit a specific Pandas concept.\n",
    "- If you want to clean up any files created by this notebook, you can also work through [16. Cleanup](#16-cleanup).\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- [1. Prerequisites](#1-prerequisites) \n",
    "- [2. The Pandas Series Object](#2-the-pandas-series-object)\n",
    "  - [2.1 Creating Pandas Series](#21-creating-pandas-series)\n",
    "  - [2.2 Indexing and Slicing in Pandas Series](#22-indexing-and-slicing-in-pandas-series)\n",
    "- [3. The Pandas DataFrame Object](#3-the-pandas-dataframe-object)\n",
    "  - [3.1 Creating Pandas DataFrames](#31-creating-pandas-dataframes)\n",
    "  - [3.2 Indexing and Slicing in Pandas DataFrames](#32-indexing-and-slicing-in-pandas-dataframes)\n",
    "- [4. The Pandas Index Object](#4-the-pandas-index-object)\n",
    "  - [4.1 Creating Pandas Index](#41-creating-pandas-index)\n",
    "  - [4.2 Indexing and Slicing in a Pandas Index](#42-indexing-and-slicing-in-a-pandas-index)\n",
    "- [5. Data Indexing and Selection in Series](#5-data-indexing-and-selection-in-series)\n",
    "  - [5.1 Dictionary Operations](#51-dictionary-operations)\n",
    "  - [5.2 List Operations](#52-list-operations)\n",
    "  - [5.3 Masking and Fancy Indexing](#53-masking-and-fancy-indexing)\n",
    "  - [5.4 Explicit and Implicit Indexing](#54-explicit-and-implicit-indexing)\n",
    "- [6. Data Indexing and Selection in DataFrames](#6-data-indexing-and-selection-in-dataframes)\n",
    "  - [6.1 Dictionary Operations](#61-dictionary-operations)\n",
    "  - [6.2 Accessing the Underlying Numpy Array](#62-accessing-the-underlying-numpy-array)\n",
    "  - [6.3 Explicit and Implicit Indexing](#63-explicit-and-implicit-indexing)\n",
    "- [7. Operating on Data in Pandas (ufuncs)](#7-operating-on-data-in-pandas-ufuncs)\n",
    "- [8. Handling Missing Values](#8-handling-missing-values)\n",
    "  - [8.1 Missing Values in Pandas Series and DataFrames](#81-missing-values-in-pandas-series-and-dataframes)\n",
    "  - [8.2 Introducing Missing Values into a Series or DataFrame](#82-introducing-missing-values-into-a-series-or-dataframe)\n",
    "  - [8.3 Detecting Missing Values](#83-detecting-missing-values)\n",
    "  - [8.4 Dropping Missing Values](#84-dropping-missing-values)\n",
    "  - [8.5 Imputing Missing Values](#85-imputing-missing-values)\n",
    "- [9. Combining Datasets: Concat](#9-combining-datasets-concat)\n",
    "  - [9.1 Concatenating Series and DataFrames](#91-concatenating-series-and-dataframes)\n",
    "  - [9.2 Concatenating Series](#92-concatenating-series)\n",
    "  - [9.3 Concatenating DataFrames](#93-concatenating-dataframes)\n",
    "  - [9.4 Concatenating Along an Axis](#94-concatenating-along-an-axis)\n",
    "  - [9.5 Index Preservation During Concatenation](#95-index-preservation-during-concatenation)\n",
    "  - [9.6 Ignoring Index Preservation During Concatenation](#96-ignoring-index-preservation-during-concatenation)\n",
    "  - [9.7 Concatenation with Outer Join](#97-concatenation-with-outer-join)\n",
    "  - [9.8 Concatenation with Inner Join](#98-concatenation-with-inner-join)\n",
    "- [10. Combining Datasets: Merge](#10-combining-datasets-merge)\n",
    "  - [10.1 Merging Series and DataFrames](#101-merging-series-and-dataframes)\n",
    "  - [10.2 One-To-One Merge Example](#102-one-to-one-merge-example)\n",
    "  - [10.3 One-To-Many Merge Example](#103-one-to-many-merge-example)\n",
    "  - [10.4 Many-To-Many Merge Example](#104-many-to-many-merge-example)\n",
    "  - [10.5 Merging on a Common Column](#105-merging-on-a-common-column)\n",
    "  - [10.6 Merging on Different DataFrame Columns](#106-merging-on-different-dataframe-columns)\n",
    "  - [10.7 Merging and Dropping Superfluous Columns](#107-merging-and-dropping-superfluous-columns)\n",
    "  - [10.8 Merging on DataFrame Indices](#108-merging-on-dataFrame-indices)\n",
    "  - [10.9 Merging on an Index and a Column](#109-merging-on-an-index-and-a-column)\n",
    "  - [10.10 Merging with Inner Join](#1010-merging-with-inner-join)\n",
    "  - [10.11 Merging with Outer Join](#1011-merging-with-outer-join)\n",
    "  - [10.12 Merging with Left Join](#1012-merging-with-left-join)\n",
    "  - [10.13 Merging with Right Join](#1013-merging-with-right-join)\n",
    "  - [10.13 Merging DataFrames with Common Non-Key Colums](#1013-merging-dataframes-with-common-non-key-colums)\n",
    "- [11. Aggregation](#11-aggregation)\n",
    "  - [11.1 Aggregation Methods](#111-aggregation-methods)\n",
    "  - [11.2 Aggregating Along an Axis](#112-aggregating-along-an-axis)\n",
    "  - [11.3 Summary Statistics with the `describe` Method](#113-summary-statistics-with-the-describe-method)\n",
    "- [12. Grouping](#12-grouping)\n",
    "  - [12.1 The Split-Apply-Merge Scheme](#121-the-split-apply-merge-scheme)\n",
    "  - [12.2 Grouping on a Column and Calculating the Sum](#122-grouping-on-a-column-and-calculating-the-sum)\n",
    "  - [12.3 Grouping on a Column and Calculating the Median](#123-grouping-on-a-column-and-calculating-the-median)\n",
    "  - [12.4 Grouping and Using the `aggregate` Method](#124-grouping-and-using-the-aggregate-method)\n",
    "  - [12.5 Grouping and Using the `filter` Method](#125-grouping-and-using-the-filter-method)\n",
    "  - [12.6 Grouping and Using the `transform` Method](#126-grouping-and-using-the-transform-method)\n",
    "  - [12.7 Grouping and Using the `apply` Method](#127-grouping-and-using-the-apply-method)\n",
    "- [13. Vectorized String Operations](#13-vectorized-string-operations)\n",
    "  - [13.1 Using the `str` attribute with String Methods](#131-using-the-str-attribute-with-string-methods)\n",
    "- [14. Working with Time Series](#14-working-with-time-series)\n",
    "  - [14.1 Date and Time Data Types in Pandas](#141-date-and-time-data-types-in-pandas)\n",
    "  - [14.2 Dates and Times in Python](#142-dates-and-times-in-python)\n",
    "  - [14.3 Dates and Times in Numpy](#143-dates-and-times-in-numpy)\n",
    "  - [14.4 Dates and Times in Pandas](#144-dates-and-times-in-pandas)\n",
    "  - [14.5 DatetimeIndex and Indexing](#145-datetimeindex-and-indexing)\n",
    "  - [14.6 to_datetime(), to_period(), and to_timedelta()](#146-to_datetime-to_period-and-to_timedelta)\n",
    "  - [14.7 date_range(), period_range(), and timedelta_range()](#147-date_range-period_range-and-timedelta_range)\n",
    "  - [14.8 Resampling a Time Series](#148-resampling-a-time-series)\n",
    "  - [14.9 Shifting a Time Series (by Value)](#149-shifting-a-time-series-by-value)\n",
    "  - [14.10 Shifting a Time Series (by Index)](#1410-shifting-a-time-series-by-index)\n",
    "  - [14.11 Rolling Aggregates](#1411-rolling-aggregates)\n",
    "- [15. Saving and Loading Series and DataFrames](#15-saving-and-loading-series-and-dataframes)\n",
    "  - [15.1 CSV, Excel, JSON, and Pickle Files](#151-csv-excel-json-and-pickle-files)\n",
    "  - [15.2 Saving a DataFrame to a CSV, Excel, JSON, and a Pickle File](#152-saving-a-dataFrame-to-a-csv-excel-json-and-a-pickle-file)\n",
    "  - [15.3 Loading a DataFrame from a CSV File](#153-loading-a-dataframe-from-a-csv-file)\n",
    "  - [15.4 Loading a DataFrame from an Excel File](#154-loading-a-dataframe-from-an-excel-file)\n",
    "  - [15.5 Loading a DataFrame from a JSON File](#155-loading-a-dataframe-from-a-json-file)\n",
    "  - [15.6 Loading a DataFrame from a Pickle File](#156-loading-a-dataframe-from-a-pickle-file)\n",
    "- [16. Cleanup](#16-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aa3e9",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Prerequisites\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc1de0",
   "metadata": {},
   "source": [
    "Let's make sure you have a working Python virtual environment.\n",
    "\n",
    "- If you don't already have a working environment, run the code below in a terminal (Windows/Linux: `Ctrl + J`, MacOS: `Cmd + J`).\n",
    "\n",
    "  ```bash\n",
    "  conda create -y -p ./.conda python=3.12\n",
    "  conda activate ./.conda\n",
    "  python -m pip install --upgrade pip\n",
    "  pip install ipykernel jupyter pylance numpy pandas matplotlib seaborn bokeh plotly\n",
    "  pip install dash dash-bootstrap-components openpyxl lxml pycountry\n",
    "  ```\n",
    "\n",
    "- Then, make sure you have chosen that environment by clicking `Select Kernel` in the top right of this Notebook.\n",
    "\n",
    "Alternatively, you can run this Notebook in Google CoLab.\n",
    "- Click [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paga-hb/C1VI1B_2025/blob/main/workshop2/pandas.ipynb)\n",
    "\n",
    "- In Google CoLab, choose `File -> Save a Copy in Drive`.\n",
    "- Now you can work through the Notebook cells in Google CoLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3231",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. The Pandas Series Object\n",
    "---\n",
    "\n",
    "- Pandas i built on top of NumPy, and provides an efficient implementation of a `Series` and `DataFrame` object, including an `Index` object.\n",
    "- A `Series` is a one-dimensional array of indexed data of one data type.\n",
    "- A `DataFrame` is a multi-dimensional array with attached row and column labels, often with heterogeneous types and/or missing data (`NaN`, `None`).\n",
    "- An `Index` object is used to index into a `Series` or `DataFrame`.\n",
    "- Pandas also implements a number of powerful data operations similar to database frameworks and spreadsheet programs.\n",
    "\n",
    "To use Pandas, we need to:\n",
    "- `pip install pandas`\n",
    "- `import pandas as pd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd8e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Creating Pandas Series\n",
    "\n",
    "- We can use `pd.Series(data)` to create a Pandas series, where `data` can be a NumPy array, a Python list, or a Python dictionary.\n",
    "- In the print out below, we see a series object wraps both a sequence of values (right column) and a sequence of indices (left column), and has a specific data type (last row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0042a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.25\n",
      "1    0.50\n",
      "2    0.75\n",
      "3    1.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.array([0.25, 0.5, 0.75, 1.0]) # NumPy array\n",
    "data = [0.25, 0.5, 0.75, 1.0]           # Python list\n",
    "data = {0:0.25, 1:0.5, 2:0.75, 3:1.0}   # Python dictionary\n",
    "\n",
    "s = pd.Series(data)                     # Pandas Series\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa245142",
   "metadata": {},
   "source": [
    "- We can explicitly set the data type using the keyword `dtype`, and can access the sequence of values and indices using the `values` and `index` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1977e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.25\n",
      "1    0.50\n",
      "2    0.75\n",
      "3    1.00\n",
      "dtype: float32\n",
      "\n",
      "[0.25 0.5  0.75 1.  ]\n",
      "RangeIndex(start=0, stop=4, step=1)\n"
     ]
    }
   ],
   "source": [
    "data = [0.25, 0.5, 0.75, 1.0]\n",
    "s = pd.Series(data, dtype=np.float32)\n",
    "print(s)\n",
    "\n",
    "print()\n",
    "\n",
    "print(s.values)\n",
    "print(s.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c599938",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Indexing and Slicing in Pandas Series\n",
    "\n",
    "- Indexing and slicing works the same way as with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef12e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.25\n",
      "1    0.50\n",
      "2    0.75\n",
      "3    1.00\n",
      "dtype: float32\n",
      "\n",
      "0.5\n",
      "\n",
      "1    0.50\n",
      "2    0.75\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "data = [0.25, 0.5, 0.75, 1.0]\n",
    "s = pd.Series(data, dtype=np.float32)\n",
    "print(s)\n",
    "\n",
    "print()\n",
    "\n",
    "print(s[1])   # indexing\n",
    "\n",
    "print()\n",
    "\n",
    "print(s[1:3]) #  slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fc9b7",
   "metadata": {},
   "source": [
    "- Pandas Series have an explicitly defined index (of any type), and can be set with the keyword `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612152d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "d    1.00\n",
      "dtype: float32\n",
      "\n",
      "0.5\n",
      "\n",
      "b    0.50\n",
      "c    0.75\n",
      "d    1.00\n",
      "dtype: float32\n",
      "\n",
      "Index(['a', 'b', 'c', 'd'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = [0.25, 0.5, 0.75, 1.0]\n",
    "index = ['a', 'b', 'c', 'd']\n",
    "s = pd.Series(data, dtype=np.float32, index=index)\n",
    "print(s)\n",
    "\n",
    "print()\n",
    "\n",
    "print(s['b'])     # indexing\n",
    "\n",
    "print()\n",
    "\n",
    "print(s['b':'d']) # slicing\n",
    "\n",
    "print()\n",
    "\n",
    "print(s.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826e3be",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. The Pandas DataFrame Object\n",
    "---\n",
    "\n",
    "- A `DataFrame` is a multi-dimensional array with attached row and column labels, often with heterogeneous types and/or missing data (`NaN`, `None`).\n",
    "- An `Index` object is used to index into a `Series` or `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd042cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1 Creating Pandas DataFrames\n",
    "\n",
    "- We can use `pd.DataFrame(data)` to create a Pandas data frame, where `data` can be a two-dimensional NumPy array, a Python list of dictionaries, or a dictionary of Series objects.\n",
    "- In the print out below, we see a data frame object wraps Series objects (2 right-most columns), a column\n",
    "Index (top row), and a row Index (left column), where the Series objects share the same row Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94c29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.array([[1,4],[2,5],[3,6]])               # 2D NumPy array\n",
    "data = [{0:1, 1:4},{0:2,1:5},{0:3,1:6}]            # List of dicts\n",
    "data = {0:pd.Series([1,2,3]),1:pd.Series([4,5,6])} # Dict of Series\n",
    "\n",
    "df = pd.DataFrame(data)                            # Pandas DataFrame\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a91667d",
   "metadata": {},
   "source": [
    "- The **row index** is accessed via the `index` attribute.\n",
    "- The **column index** is accessed via the `columns` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d02aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n",
      "\n",
      "RangeIndex(start=0, stop=3, step=1)\n",
      "\n",
      "Index([0, 1], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "data = {0:pd.Series([1,2,3]), 1:pd.Series([4,5,6])}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.index)\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c0b9c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 Indexing and Slicing in Pandas DataFrames\n",
    "\n",
    "- Indexing into a `DataFrame` is done via the **column index**, returning a `Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e27501f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n",
      "\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "0    4\n",
      "1    5\n",
      "2    6\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = {0:pd.Series([1,2,3]), 1:pd.Series([4,5,6])}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print(df[0])\n",
    "\n",
    "print()\n",
    "\n",
    "print(df[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4928295",
   "metadata": {},
   "source": [
    "- Each `Series` object has its own data type and can specify any type for its **column index**.\n",
    "- Notice the `dtype` for the DataFrame's **column index** is `'object'` since it contains a string `'a'` and an int `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec1c4e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a  1\n",
      "0  1.0  4\n",
      "1  2.0  5\n",
      "2  3.0  6\n",
      "\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "Name: a, dtype: float64\n",
      "\n",
      "RangeIndex(start=0, stop=3, step=1)\n",
      "\n",
      "Index(['a', 1], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = {'a':pd.Series([1.0,2.0,3.0]), 1:pd.Series([4,5,6])}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print(df['a'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.index)\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d410f84",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. The Pandas Index Object\n",
    "---\n",
    "\n",
    "- An `Index` object is used to index into a `Series` or `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec50c6f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.1 Creating Pandas Index\n",
    "\n",
    "- We can use `pd.Index(sequence)` to create a Pandas `Index`, where `sequence` can be any sequence, e.g. a NumPy array, a Python list, etc., and the sequence doesn’t have to be consecutive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa46668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([2, 3, 5, 7, 11], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "sequence = ['b', 'c', 'e', 'g', 'k']  # Python list of strings\n",
    "sequence = [2.0, 3.0, 5.0, 7.0, 11.0] # Python list of floats\n",
    "sequence = [2, 3, 5, 7, 11]           # Python list or ints\n",
    "sequence = np.array([2, 3, 5, 7, 11]) # NumPy array of ints\n",
    "\n",
    "ind = pd.Index(sequence)              # Pandas Index\n",
    "\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205cc77",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.2 Indexing and Slicing in a Pandas Index\n",
    "\n",
    "- Indexing and slicing works the same way as with a NumPy array, but a Pandas `Index` is immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dda8fb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([2, 3, 5, 7, 11], dtype='int64')\n",
      "\n",
      "3\n",
      "\n",
      "Index([2, 5, 11], dtype='int64')\n",
      "\n",
      "5\n",
      "(5,)\n",
      "1\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "sequence = [2, 3, 5, 7, 11]\n",
    "ind = pd.Index(sequence)\n",
    "print(ind)\n",
    "\n",
    "print()\n",
    "\n",
    "print(ind[1])\n",
    "\n",
    "print()\n",
    "\n",
    "print(ind[::2])\n",
    "\n",
    "print()\n",
    "\n",
    "print(ind.size)\n",
    "print(ind.shape)\n",
    "print(ind.ndim)\n",
    "print(ind.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab28fd4",
   "metadata": {},
   "source": [
    "- A Pandas `Index` also works with set operations, such as intersection (`&`), union (`|`), symmetric\n",
    "difference (`^`), and set difference using the `difference()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "036bb241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([1, 3, 5, 7, 9], dtype='int64')\n",
      "Index([2, 3, 5, 7, 11], dtype='int64')\n",
      "\n",
      "Index([0, 3, 5, 7, 9], dtype='int64')\n",
      "Index([3, 3, 5, 7, 11], dtype='int64')\n",
      "Index([3, 0, 0, 0, 2], dtype='int64')\n",
      "Index([1, 9], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])\n",
    "print(indA)\n",
    "print(indB)\n",
    "\n",
    "print()\n",
    "\n",
    "print(indA & indB) # intersection\n",
    "print(indA | indB) # union\n",
    "print(indA ^ indB) # symmetric difference\n",
    "print(indA.difference(indB)) # set difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbff542",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Data Indexing and Selection in Series\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db431687",
   "metadata": {},
   "source": [
    "## 5.1 Dictionary Operations\n",
    "\n",
    "- `Series` objects support dictionary operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b678c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "dtype: float64\n",
      "\n",
      "0.5\n",
      "\n",
      "True\n",
      "\n",
      "Index(['a', 'b', 'c'], dtype='object')\n",
      "\n",
      "[('a', 0.25), ('b', 0.5), ('c', 0.75)]\n",
      "\n",
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "d    1.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75], index=['a', 'b', 'c'])\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data['b'])\n",
    "print()\n",
    "\n",
    "print('a' in data)\n",
    "print()\n",
    "\n",
    "print(data.keys())\n",
    "print()\n",
    "\n",
    "print(list(data.items()))\n",
    "print()\n",
    "\n",
    "data['d'] = 1.00\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e098c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.2 List Operations\n",
    "\n",
    "- `Series` objects support list operations, such as slicing.\n",
    "- Note that when using the **explicit index**, the end point is included, whereas it isn’t in the **implicit integer index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a913c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "d    1.00\n",
      "dtype: float64\n",
      "\n",
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "dtype: float64\n",
      "\n",
      "a    0.25\n",
      "b    0.50\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data['a':'c'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109efc8",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.3 Masking and Fancy Indexing\n",
    "\n",
    "- `Series` objects also support masking and fancy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdfabd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0.25\n",
      "b    0.50\n",
      "c    0.75\n",
      "d    1.00\n",
      "dtype: float64\n",
      "\n",
      "b    0.50\n",
      "c    0.75\n",
      "dtype: float64\n",
      "\n",
      "a    0.25\n",
      "d    1.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data[(data > 0.3) & (data < 0.8)]) # masking\n",
    "\n",
    "print()\n",
    "\n",
    "print(data[['a', 'd']]) # fancy indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88dd9bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.4 Explicit and Implicit Indexing\n",
    "\n",
    "- Confusion between **explicit** and **implicit** indexing arises when using an **explicit integer index**.\n",
    "  - The **explicit index** is used when indexing.\n",
    "  - The **implicit integer index** is used when slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d6422cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    a\n",
      "3    b\n",
      "5    c\n",
      "dtype: object\n",
      "\n",
      "a\n",
      "\n",
      "3    b\n",
      "5    c\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data[1])   # <- uses explicit index when indexing\n",
    "\n",
    "print()\n",
    "\n",
    "print(data[1:3]) # <- uses implicit index when slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c1c0d",
   "metadata": {},
   "source": [
    "- The attributes `loc` and `iloc` remove the confusion between explicit and implicit indexing.\n",
    "  - The `loc` attribute always uses **explicit indexing**.\n",
    "  - The `iloc` attribute always uses **implicit integer indexing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d042fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    a\n",
      "3    b\n",
      "5    c\n",
      "dtype: object\n",
      "\n",
      "a\n",
      "\n",
      "1    a\n",
      "3    b\n",
      "dtype: object\n",
      "\n",
      "b\n",
      "\n",
      "3    b\n",
      "5    c\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data.loc[1])    # <- uses explicit indexing\n",
    "print()\n",
    "print(data.loc[1:3])  # <- uses explicit indexing\n",
    "\n",
    "print()\n",
    "\n",
    "print(data.iloc[1])   # <- uses implicit indexing\n",
    "print()\n",
    "print(data.iloc[1:3]) # <- uses implicit indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ddff1",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Data Indexing and Selection in DataFrames\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee9c4e",
   "metadata": {},
   "source": [
    "## 6.1 Dictionary Operations\n",
    "\n",
    "- `DataFrame` objects support dictionary operations, such as indexing using the column index (column name).\n",
    "- This returns the column from the `DataFrame`, which is a `Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d578f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              area       pop\n",
      "California  423967  38332521\n",
      "Texas       695662  26448193\n",
      "New York    141297  19651127\n",
      "Florida     170312  19552860\n",
      "Illinois    149995  12882135\n",
      "\n",
      "California    423967\n",
      "Texas         695662\n",
      "New York      141297\n",
      "Florida       170312\n",
      "Illinois      149995\n",
      "Name: area, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "area = pd.Series({'California': 423967,\n",
    "                  'Texas': 695662,\n",
    "                  'New York': 141297,\n",
    "                  'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "\n",
    "pop = pd.Series({'California': 38332521,\n",
    "                 'Texas': 26448193,\n",
    "                 'New York': 19651127,\n",
    "                 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print(data['area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da8b2c",
   "metadata": {},
   "source": [
    "- We can also index by column index (column name) and assign a value (adds a new column to the `DataFrame` if the column index doesn’t exist).\n",
    "- We can also perform vector calculations using the `DataFrame`’s columns (`Series` objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b48aac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              area       pop\n",
      "California  423967  38332521\n",
      "Texas       695662  26448193\n",
      "New York    141297  19651127\n",
      "Florida     170312  19552860\n",
      "Illinois    149995  12882135\n",
      "\n",
      "              area       pop     density\n",
      "California  423967  38332521   90.413926\n",
      "Texas       695662  26448193   38.018740\n",
      "New York    141297  19651127  139.076746\n",
      "Florida     170312  19552860  114.806121\n",
      "Illinois    149995  12882135   85.883763\n"
     ]
    }
   ],
   "source": [
    "area = pd.Series({'California': 423967,\n",
    "                  'Texas': 695662,\n",
    "                  'New York': 141297,\n",
    "                  'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "\n",
    "pop = pd.Series({'California': 38332521,\n",
    "                 'Texas': 26448193,\n",
    "                 'New York': 19651127,\n",
    "                 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "data['density'] = data['pop'] / data['area']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83888d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.2 Accessing the Underlying Numpy Array\n",
    "\n",
    "- We can access the underlying NumPy array via the `DataFrame`’s `values` attribute.\n",
    "- We can transpose (swap column and rows) the `DataFrame` via the `T` attribute.\n",
    "- Indexing with a single value into a NumPy array (via the `values` attribute) returns a row.\n",
    "- Indexing with a single value into a `DataFrame` returns a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54cef0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              area       pop     density\n",
      "California  423967  38332521   90.413926\n",
      "Texas       695662  26448193   38.018740\n",
      "New York    141297  19651127  139.076746\n",
      "Florida     170312  19552860  114.806121\n",
      "Illinois    149995  12882135   85.883763\n",
      "\n",
      "[[4.23967000e+05 3.83325210e+07 9.04139261e+01]\n",
      " [6.95662000e+05 2.64481930e+07 3.80187404e+01]\n",
      " [1.41297000e+05 1.96511270e+07 1.39076746e+02]\n",
      " [1.70312000e+05 1.95528600e+07 1.14806121e+02]\n",
      " [1.49995000e+05 1.28821350e+07 8.58837628e+01]]\n",
      "\n",
      "           California         Texas      New York       Florida      Illinois\n",
      "area     4.239670e+05  6.956620e+05  1.412970e+05  1.703120e+05  1.499950e+05\n",
      "pop      3.833252e+07  2.644819e+07  1.965113e+07  1.955286e+07  1.288214e+07\n",
      "density  9.041393e+01  3.801874e+01  1.390767e+02  1.148061e+02  8.588376e+01\n",
      "\n",
      "[4.23967000e+05 3.83325210e+07 9.04139261e+01]\n",
      "\n",
      "California    423967\n",
      "Texas         695662\n",
      "New York      141297\n",
      "Florida       170312\n",
      "Illinois      149995\n",
      "Name: area, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "area = pd.Series({'California': 423967,\n",
    "                  'Texas': 695662,\n",
    "                  'New York': 141297,\n",
    "                  'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521,\n",
    "                 'Texas': 26448193,\n",
    "                 'New York': 19651127,\n",
    "                 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data['density'] = data['pop'] / data['area']\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.values)    # underlying numpy array\n",
    "print()\n",
    "\n",
    "print(data.T)         # transpose\n",
    "print()\n",
    "\n",
    "print(data.values[0]) # index into underlying numpy array\n",
    "print()\n",
    "\n",
    "print(data['area'])   # select a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58ef04",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.3 Explicit and Implicit Indexing\n",
    "\n",
    "- We can use **implicit** and **explicit** indexing via the `iloc` and `loc` attributes.\n",
    "- We can also combine masking and fancy indexing via the `loc` attribute.\n",
    "- We can also assign values to the `DataFrame` using any indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c90109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              area       pop     density\n",
      "California  423967  38332521   90.413926\n",
      "Texas       695662  26448193   38.018740\n",
      "New York    141297  19651127  139.076746\n",
      "Florida     170312  19552860  114.806121\n",
      "Illinois    149995  12882135   85.883763\n",
      "\n",
      "              area       pop\n",
      "California  423967  38332521\n",
      "Texas       695662  26448193\n",
      "New York    141297  19651127\n",
      "\n",
      "              area       pop\n",
      "California  423967  38332521\n",
      "Texas       695662  26448193\n",
      "New York    141297  19651127\n",
      "Florida     170312  19552860\n",
      "Illinois    149995  12882135\n",
      "\n",
      "               pop     density\n",
      "New York  19651127  139.076746\n",
      "Florida   19552860  114.806121\n",
      "\n",
      "              area       pop     density\n",
      "California  423967  38332521   90.000000\n",
      "Texas       695662  26448193   38.018740\n",
      "New York    141297  19651127  139.076746\n",
      "Florida     170312  19552860  114.806121\n",
      "Illinois    149995  12882135   85.883763\n"
     ]
    }
   ],
   "source": [
    "area = pd.Series({'California': 423967,\n",
    "                  'Texas': 695662,\n",
    "                  'New York': 141297,\n",
    "                  'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521,\n",
    "                 'Texas': 26448193,\n",
    "                 'New York': 19651127,\n",
    "                 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data['density'] = data['pop'] / data['area']\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.iloc[:3, :2]) # implicit indexing\n",
    "print()\n",
    "\n",
    "print(data.loc[:'Illinois', :'pop']) # explicit indexing\n",
    "print()\n",
    "\n",
    "print(data.loc[data.density > 100, ['pop', 'density']]) # explicit indexing with fancy indexing\n",
    "print()\n",
    "\n",
    "data.iloc[0, 2] = 90\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a7253",
   "metadata": {},
   "source": [
    "- Additional indexing conventions are:\n",
    "  - While indexing refers to columns, slicing (**explicit**, **implicit**) refers to rows.\n",
    "  - Direct masking operations are also interpreted as row-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0ce7d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              area       pop     density\n",
      "California  423967  38332521   90.413926\n",
      "Texas       695662  26448193   38.018740\n",
      "New York    141297  19651127  139.076746\n",
      "Florida     170312  19552860  114.806121\n",
      "Illinois    149995  12882135   85.883763\n",
      "\n",
      "            area       pop     density\n",
      "Florida   170312  19552860  114.806121\n",
      "Illinois  149995  12882135   85.883763\n",
      "\n",
      "            area       pop     density\n",
      "Texas     695662  26448193   38.018740\n",
      "New York  141297  19651127  139.076746\n",
      "\n",
      "            area       pop     density\n",
      "New York  141297  19651127  139.076746\n",
      "Florida   170312  19552860  114.806121\n"
     ]
    }
   ],
   "source": [
    "area = pd.Series({'California': 423967,\n",
    "                  'Texas': 695662,\n",
    "                  'New York': 141297,\n",
    "                  'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521,\n",
    "                 'Texas': 26448193,\n",
    "                 'New York': 19651127,\n",
    "                 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data['density'] = data['pop'] / data['area']\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data['Florida':'Illinois'])\n",
    "print()\n",
    "\n",
    "print(data[1:3])\n",
    "print()\n",
    "\n",
    "print(data[data.density > 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a669a81",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Operating on Data in Pandas (ufuncs)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9e9fa",
   "metadata": {},
   "source": [
    "- NumPy universal functions (ufuncs) work with Pandas Series and DataFrames\n",
    "  - Python’s operators have been overloaded by Pandas with equivalent ufuncs.\n",
    "  - Unary operators and ufuncs preserve Pandas indexes.\n",
    "  - Binary operators and ufuncs align Pandas indexes in the two objects.\n",
    "\n",
    "<img src=\"../images/pandas-ufuncs.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6480cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n",
      "\n",
      "0    4\n",
      "1    5\n",
      "2    6\n",
      "dtype: int64\n",
      "\n",
      "0   -1\n",
      "1   -2\n",
      "2   -3\n",
      "dtype: int64\n",
      "\n",
      "0    5\n",
      "1    7\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s1 = pd.Series([1,2,3])\n",
    "s2 = pd.Series([4,5,6])\n",
    "print(s1)\n",
    "print()\n",
    "print(s2)\n",
    "\n",
    "print()\n",
    "\n",
    "s3 = -s1\n",
    "print(s3)\n",
    "print()\n",
    "\n",
    "s4 = s1 + s2\n",
    "s4 = s1.add(s2)\n",
    "print(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85733f17",
   "metadata": {},
   "source": [
    "- When performing binary operations on two `Series` objects or `DataFrame` objects with different sets of indexes, Pandas will:\n",
    "  - Form the union of the indexes.\n",
    "  - Fill elements with the value NaN (Not a Number) for any non-aligned indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "feb9a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  2\n",
      "1  3  4\n",
      "\n",
      "   B  A  C\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n",
      "\n",
      "     A    B   C\n",
      "0  3.0  3.0 NaN\n",
      "1  8.0  8.0 NaN\n",
      "2  NaN  NaN NaN\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([[1,2],[3,4]], columns=['A', 'B'])\n",
    "print(df1)\n",
    "print()\n",
    "\n",
    "df2 = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns=['B','A','C'])\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "df3 = df1 + df2\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59717c0",
   "metadata": {},
   "source": [
    "- If we want to replace `NaN` with another value, we can use the `fillna()` method.\n",
    "  - The keyword `inplace` set to `True` will replace the values in the `DataFrame` object instead of returning a `DataFrame` copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4824fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  2\n",
      "1  3  4\n",
      "\n",
      "   B  A  C\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n",
      "\n",
      "     A    B    C\n",
      "0  3.0  3.0  0.0\n",
      "1  8.0  8.0  0.0\n",
      "2  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([[1,2],[3,4]], columns=['A', 'B'])\n",
    "print(df1)\n",
    "print()\n",
    "\n",
    "df2 = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns=['B','A','C'])\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "df3 = df1 + df2\n",
    "df3.fillna(0.0, inplace=True)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591668a",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Handling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dae8d4",
   "metadata": {},
   "source": [
    "## 8.1 Missing Values in Pandas Series and DataFrames\n",
    "\n",
    "- Real-world data often contains missing values, i.e. Not Available (NA) values.\n",
    "- In `Series` and `DataFrame` objects they are represented with, either:\n",
    "    - `NaN` (which is NumPy’s `np.nan` value) for `float` data types.\n",
    "    - `None` (which is Python’s built-in `None` value) for `object` data types.\n",
    "    - `<NA>` (which is Pandas’ built-in `pd.NA` value) for non-numeric data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe96e18",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.2 Introducing Missing Values into a Series or DataFrame\n",
    "\n",
    "- Introducing missing values into a `Series` or `DataFrame` column will:\n",
    "  - Convert numeric data types (e.g. `int32`) to `float64`.\n",
    "  - Convert non-numeric data types (e.g. `bool` or `str`) to `object`.\n",
    "\n",
    "<img src=\"../images/pandas-nans.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8eac71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "0    NaN\n",
      "1    2.0\n",
      "2    NaN\n",
      "3    4.0\n",
      "4    NaN\n",
      "dtype: float64\n",
      "\n",
      "bool\n",
      "0      NaN\n",
      "1     True\n",
      "2     None\n",
      "3    False\n",
      "4     <NA>\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "s = pd.Series([1, 2, 3, 4, 5], dtype=np.int32)\n",
    "print(s.dtype)\n",
    "s[0] = np.nan\n",
    "s[2] = None\n",
    "s[4] = pd.NA\n",
    "print(s)\n",
    "\n",
    "print()\n",
    "\n",
    "s = pd.Series([True, True, True, False, False])\n",
    "print(s.dtype)\n",
    "s[0] = np.nan\n",
    "s[2] = None\n",
    "s[4] = pd.NA\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79489874",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.3 Detecting Missing Values\n",
    "\n",
    "- Detecting missing values with `isnull()` and `notnull()\n",
    "  - `isnull()` returns a boolean mask indicating missing values with `True`.\n",
    "  - `notnull()` is the opposite of `isnull()` (`False` for missing values).`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13f1f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1      NaN\n",
      "2    hello\n",
      "3     None\n",
      "dtype: object\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "dtype: bool\n",
      "\n",
      "0        1\n",
      "2    hello\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.isnull())\n",
    "print()\n",
    "\n",
    "print(data[data.notnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0c196",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.4 Dropping Missing Values\n",
    "\n",
    "- Dropping missing values in a `Series` object with `dropna()`.\n",
    "  - `dropna()` returns a filtered (missing values removed) copy of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60328980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1      NaN\n",
      "2    hello\n",
      "3     None\n",
      "dtype: object\n",
      "\n",
      "0        1\n",
      "2    hello\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed143e",
   "metadata": {},
   "source": [
    "- Dropping missing values in a `DataFrame` object with `dropna()`.\n",
    "    - `dropna()` will drop entire rows or columns containing at least one missing value.\n",
    "    - By default `dropna()` uses `dropna(axis=0)` which is the same as `dropna(axis='rows')`\n",
    "    - `dropna(axis=1)` is the same as `dropna(axis='columns')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b90a892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1  2\n",
      "0  1.0  NaN  2\n",
      "1  2.0  3.0  5\n",
      "2  NaN  4.0  6\n",
      "\n",
      "     0    1  2\n",
      "1  2.0  3.0  5\n",
      "\n",
      "   2\n",
      "0  2\n",
      "1  5\n",
      "2  6\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame([[1, np.nan, 2], [2, 3, 5], [np.nan, 4, 6]])\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.dropna())\n",
    "print()\n",
    "\n",
    "print(data.dropna(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b12200",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.5 Imputing Missing Values\n",
    "\n",
    "- Filling (replacing) missing values in a `Series` object with `fillna()`, `ffill()`, and `bfill()`.\n",
    "  - `fillna(value)` replaces all missing values with the provided `value`.\n",
    "  - `ffill()` is a forward fill that replaces a missing value with the **preceding** value in the sequence of values.\n",
    "  - `bfill()` is a backward fill that replaces a missing value with the **succeeding** value in the sequence of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2e7852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1.0\n",
      "b    NaN\n",
      "c    2.0\n",
      "d    NaN\n",
      "e    3.0\n",
      "dtype: float64\n",
      "\n",
      "a    1.0\n",
      "b    1.0\n",
      "c    2.0\n",
      "d    2.0\n",
      "e    3.0\n",
      "dtype: float64\n",
      "\n",
      "a    1.0\n",
      "b    2.0\n",
      "c    2.0\n",
      "d    3.0\n",
      "e    3.0\n",
      "dtype: float64\n",
      "\n",
      "a    1.0\n",
      "b    0.0\n",
      "c    2.0\n",
      "d    0.0\n",
      "e    3.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.ffill())\n",
    "print()\n",
    "\n",
    "print(data.bfill())\n",
    "print()\n",
    "\n",
    "print(data.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413ea99",
   "metadata": {},
   "source": [
    "- Filling (replacing) missing values in a `DataFrame` object with `fillna()`, `ffill()`, and `bfill()`.\n",
    "    - `fillna(value)`, `ffill()`, and `bfill()` work as with `Series`, but also support the keyword parameter `axis`.\n",
    "    - `axis=0` (default) the same as `axis='rows'`\n",
    "    - `axis=1` is the same as `axis='columns'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0aaa7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1  2   3\n",
      "0  1.0  NaN  2 NaN\n",
      "1  2.0  3.0  5 NaN\n",
      "2  NaN  4.0  6 NaN\n",
      "\n",
      "     0    1  2    3\n",
      "0  1.0  0.0  2  0.0\n",
      "1  2.0  3.0  5  0.0\n",
      "2  0.0  4.0  6  0.0\n",
      "\n",
      "     0    1    2    3\n",
      "0  1.0  1.0  2.0  2.0\n",
      "1  2.0  3.0  5.0  5.0\n",
      "2  NaN  4.0  6.0  6.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame([[1, np.nan, 2, np.nan], [2, 3, 5, np.nan], [np.nan, 4, 6, np.nan]])\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(data.fillna(0))\n",
    "print()\n",
    "\n",
    "print(data.ffill(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada9c56",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Combining Datasets: Concat\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4710b6",
   "metadata": {},
   "source": [
    "## 9.1 Concatenating Series and DataFrames\n",
    "\n",
    "- Pandas provides methods for concatenating `Series` and `DataFrames`.\n",
    "  - `concat(s1, s2)` takes two `Series` and returns their concatenation.\n",
    "  - `concat(df1, df2)` takes two `DataFrames` and returns their concatenation.\n",
    "  - The `axis` parameter determines the axis used during the concatenation.\n",
    "    - `axis=0` (`axis='rows'`) or `axis=1` (`axis='columns'`).\n",
    "  - The `ignore_index=True` parameter creates a new index after the join.\n",
    "    - `ignore_index=False` or `ignore_index=True`.\n",
    "  - The `join` parameter determines the type of join.\n",
    "    - `join='outer'` or `join='inner'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925ac8f",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.2 Concatenating Series\n",
    "\n",
    "- Concatenating two `Series` with `concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fce9a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    A\n",
      "2    B\n",
      "dtype: object\n",
      "\n",
      "3    C\n",
      "4    D\n",
      "dtype: object\n",
      "\n",
      "1    A\n",
      "2    B\n",
      "3    C\n",
      "4    D\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1 = pd.Series(['A', 'B'], index=[1, 2])\n",
    "s2 = pd.Series(['C', 'D'], index=[3, 4])\n",
    "res = pd.concat([s1, s2])\n",
    "\n",
    "print(f'{s1}\\n')\n",
    "print(f'{s2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db3c45",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.3 Concatenating DataFrames\n",
    "\n",
    "- Concatenating two `DataFrames` with `concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c59f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "1  A1  B1\n",
      "2  A2  B2\n",
      "\n",
      "    A   B\n",
      "3  A3  B3\n",
      "4  A4  B4\n",
      "\n",
      "    A   B\n",
      "1  A1  B1\n",
      "2  A2  B2\n",
      "3  A3  B3\n",
      "4  A4  B4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A1','B1'],['A2','B2']], index=[1,2], columns=['A','B'])\n",
    "df2 = pd.DataFrame([['A3','B3'],['A4','B4']], index=[3,4], columns=['A','B'])\n",
    "res = pd.concat([df1, df2])\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76713d0",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.4 Concatenating Along an Axis\n",
    "\n",
    "- The `axis` parameter determines the axis used during concatenating `DataFrames`.\n",
    "    - `axis=0` or `axis='rows'` (default) concatenates the rows.\n",
    "    - `axis=1` or `axis='columns'` concatenates the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1aeca12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "\n",
      "    C   D\n",
      "0  C0  D0\n",
      "1  C1  D1\n",
      "\n",
      "    A   B   C   D\n",
      "0  A0  B0  C0  D0\n",
      "1  A1  B1  C1  D1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A0','B0'],['A1','B1']], index=[0,1], columns=['A','B'])\n",
    "df2 = pd.DataFrame([['C0','D0'],['C1','D1']], index=[0,1], columns=['C','D'])\n",
    "res = pd.concat([df1, df2], axis='columns')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5fa41",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.5 Index Preservation During Concatenation\n",
    "\n",
    "- Pandas preserves **indices** in the `DataFrames`, by default, even if it means they are duplicated.\n",
    "  - Notice duplicated row indicies in the result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b76c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "\n",
      "    A   B\n",
      "0  A2  B2\n",
      "1  A3  B3\n",
      "\n",
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "0  A2  B2\n",
      "1  A3  B3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A0','B0'],['A1','B1']], index=[0,1], columns=['A','B'])\n",
    "df2 = pd.DataFrame([['A2','B2'],['A3','B3']], index=[0,1], columns=['A','B'])\n",
    "res = pd.concat([df1, df2])\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb71e00",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.6 Ignoring Index Preservation During Concatenation\n",
    "\n",
    "- To ignore the index, and let Pandas generate a new one, the `ignore_index` parameter can be set to `True`.\n",
    "  - Notice unique row indicies in the result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea0efb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "\n",
      "    A   B\n",
      "0  A2  B2\n",
      "1  A3  B3\n",
      "\n",
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "2  A2  B2\n",
      "3  A3  B3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A0','B0'],['A1','B1']], index=[0,1], columns=['A','B'])\n",
    "df2 = pd.DataFrame([['A2','B2'],['A3','B3']], index=[0,1], columns=['A','B'])\n",
    "res = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73867b03",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.7 Concatenation with Outer Join\n",
    "\n",
    "- The type of join performed during the concatenation can be specified using the `join` parameter.\n",
    "  - `join='outer'` (**default**) performs an outer join (missing values after the join are filled with `NaN`).\n",
    "  - `join='inner'` performs an inner join (only matching columns are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fd8f4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C\n",
      "1  A1  B1  C1\n",
      "2  A2  B2  C2\n",
      "\n",
      "    B   C   D\n",
      "3  B3  C3  D3\n",
      "4  B4  C4  D4\n",
      "\n",
      "     A   B   C    D\n",
      "0   A1  B1  C1  NaN\n",
      "1   A2  B2  C2  NaN\n",
      "2  NaN  B3  C3   D3\n",
      "3  NaN  B4  C4   D4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A1','B1','C1'],['A2','B2','C2']], index=[1,2], columns=['A','B','C'])\n",
    "df2 = pd.DataFrame([['B3','C3','D3'],['B4','C4','D4']], index=[3,4], columns=['B','C','D'])\n",
    "res = pd.concat([df1, df2], ignore_index=True)\n",
    "# res = pd.concat([df1, df2], ignore_index=True, join='outer')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e809e",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.8 Concatenation with Inner Join\n",
    "\n",
    "- The type of join performed during the concatenation can be specified using the `join` parameter.\n",
    "  - `join='outer'` (**default**) performs an outer join (missing values after the join are filled with `NaN`).\n",
    "  - `join='inner'` performs an inner join (only matching columns are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcc8944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C\n",
      "1  A1  B1  C1\n",
      "2  A2  B2  C2\n",
      "\n",
      "    B   C   D\n",
      "3  B3  C3  D3\n",
      "4  B4  C4  D4\n",
      "\n",
      "    B   C\n",
      "1  B1  C1\n",
      "2  B2  C2\n",
      "3  B3  C3\n",
      "4  B4  C4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame([['A1','B1','C1'],['A2','B2','C2']], index=[1,2], columns=['A','B','C'])\n",
    "df2 = pd.DataFrame([['B3','C3','D3'],['B4','C4','D4']], index=[3,4], columns=['B','C','D'])\n",
    "res = pd.concat([df1, df2], join='inner')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c07c8",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Combining Datasets: Merge\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869efa8",
   "metadata": {},
   "source": [
    "## 10.1 Merging Series and DataFrames\n",
    "\n",
    "- The `merge()` function implements relational logic for `Series` and `DataFrames`.\n",
    "  - One-to-one, one-to-many, and many-to-many joins.\n",
    "  - Joining on a common column with `on='col'`.\n",
    "  - Joining on a left and right column with\n",
    "  `left_on='l_col'` and `right_on='r_col'`.\n",
    "  - Joining on a the left index and right index with `left_index=True` and `right_index=True`.\n",
    "  - Performing an inner, outer, left or right join with\n",
    "  `how='inner'`, `how='outer'`, `how='left'`, or `how='right'`.\n",
    "  - Choosing suffixes for non-key columns with the same name with `suffixes=['_L','_R']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e58c8",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.2 One-To-One Merge Example\n",
    "\n",
    "- A one-to-one join of two `DataFrames` with `merge()`.\n",
    "  - One-to-one, since there's a one-to-one match between the two datasets' *primary key columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12cd0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp\n",
      "0  Al  Fin\n",
      "1  Ed  Eng\n",
      "2  Jo  Eng\n",
      "3  Bo   HR\n",
      "\n",
      "  emp  date\n",
      "0  Jo  2004\n",
      "1  Al  2008\n",
      "2  Ed  2012\n",
      "3  Bo  2014\n",
      "\n",
      "  emp  grp  date\n",
      "0  Al  Fin  2008\n",
      "1  Ed  Eng  2012\n",
      "2  Jo  Eng  2004\n",
      "3  Bo   HR  2014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df2=pd.DataFrame([['Jo',2004],['Al',2008],['Ed',2012],['Bo',2014]], columns=['emp','date'])\n",
    "res=pd.merge(df1, df2)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76aa7",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.3 One-To-Many Merge Example\n",
    "\n",
    "- A one-to-many join of two `DataFrames` with `merge()`.\n",
    "  - One-to-many, since there's a one-to-many match between the two datasets' *primary key columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e2940af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp  date\n",
      "0  Al  Fin  2008\n",
      "1  Ed  Eng  2012\n",
      "2  Jo  Eng  2004\n",
      "3  Bo   HR  2014\n",
      "\n",
      "   grp  mgr\n",
      "0  Fin  Bob\n",
      "1  Eng  Amy\n",
      "2   HR  Sue\n",
      "\n",
      "  emp  grp  date  mgr\n",
      "0  Al  Fin  2008  Bob\n",
      "1  Ed  Eng  2012  Amy\n",
      "2  Jo  Eng  2004  Amy\n",
      "3  Bo   HR  2014  Sue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin',2008],['Ed','Eng',2012],['Jo','Eng',2004],['Bo','HR',2014]], columns=['emp','grp','date'])\n",
    "df2=pd.DataFrame([['Fin','Bob'],['Eng','Amy'],['HR','Sue']], columns=['grp','mgr'])\n",
    "res=pd.merge(df1, df2)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c352638",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.4 Many-To-Many Merge Example\n",
    "\n",
    "- A many-to-many join of two `DataFrames` with `merge()`.\n",
    "  - Many-to-many, since there's a many-to-many match between the two datasets' *primary key columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9fc65f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp\n",
      "0  Al  Fin\n",
      "1  Ed  Eng\n",
      "2  Jo  Eng\n",
      "3  Bo   HR\n",
      "\n",
      "   grp skill\n",
      "0  Fin     r\n",
      "1  Fin    xl\n",
      "2  Eng     c\n",
      "3  Eng    py\n",
      "4   HR    xl\n",
      "5   HR     o\n",
      "\n",
      "  emp  grp skill\n",
      "0  Al  Fin     r\n",
      "1  Al  Fin    xl\n",
      "2  Ed  Eng     c\n",
      "3  Ed  Eng    py\n",
      "4  Jo  Eng     c\n",
      "5  Jo  Eng    py\n",
      "6  Bo   HR    xl\n",
      "7  Bo   HR     o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df2=pd.DataFrame([['Fin','r'],['Fin','xl'],['Eng','c'],['Eng','py'],['HR','xl'],['HR','o']], columns=['grp','skill'])\n",
    "res=pd.merge(df1, df2)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7da4d",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.5 Merging on a Common Column\n",
    "\n",
    "- Joining on a common column in `DataFrames` with `merge()` and the parameter `on='col'`.\n",
    "  - In this case, the `emp` column in the two `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "318ce32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp\n",
      "0  Al  Fin\n",
      "1  Ed  Eng\n",
      "2  Jo  Eng\n",
      "3  Bo   HR\n",
      "\n",
      "  emp  date\n",
      "0  Jo  2004\n",
      "1  Al  2008\n",
      "2  Ed  2012\n",
      "3  Bo  2014\n",
      "\n",
      "  emp  grp  date\n",
      "0  Al  Fin  2008\n",
      "1  Ed  Eng  2012\n",
      "2  Jo  Eng  2004\n",
      "3  Bo   HR  2014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df2=pd.DataFrame([['Jo',2004],['Al',2008],['Ed',2012],['Bo',2014]], columns=['emp','date'])\n",
    "res=pd.merge(df1, df2, on='emp')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aac13f",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.6 Merging on Different DataFrame Columns\n",
    "\n",
    "- Joining on a column in the left and right `DataFrames` with `merge()` and `left_on='l_col'`, `right_on='r_col'`.\n",
    "  - In this case, on the `emp` column in left (first) `DataFrame` and on the `name` column in right (second) `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44ec9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp\n",
      "0  Al  Fin\n",
      "1  Ed  Eng\n",
      "2  Jo  Eng\n",
      "3  Bo   HR\n",
      "\n",
      "  name  salary\n",
      "0   Al    7000\n",
      "1   Ed    8000\n",
      "2   Jo   12000\n",
      "3   Bo    9000\n",
      "\n",
      "  emp  grp name  salary\n",
      "0  Al  Fin   Al    7000\n",
      "1  Ed  Eng   Ed    8000\n",
      "2  Jo  Eng   Jo   12000\n",
      "3  Bo   HR   Bo    9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df2=pd.DataFrame([['Al',7000],['Ed',8000],['Jo',12000],['Bo',9000]], columns=['name','salary'])\n",
    "res=pd.merge(df1, df2, left_on='emp', right_on='name')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c95b7",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.7 Merging and Dropping Superfluous Columns\n",
    "\n",
    "- Joining on a column in the left and right `DataFrames` with `merge()` and `left_on='l_col'`, `right_on='r_col'`.\n",
    "  - In this case, on the `emp` column in left (first) `DataFrame` and on the `name` column in right (second) `DataFrame`.\n",
    "- Additionally, dropping superfluous columns with `drop('col', axis=1)`.\n",
    "  - In this case, the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "306614f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  emp  grp\n",
      "0  Al  Fin\n",
      "1  Ed  Eng\n",
      "2  Jo  Eng\n",
      "3  Bo   HR\n",
      "\n",
      "  name  salary\n",
      "0   Al    7000\n",
      "1   Ed    8000\n",
      "2   Jo   12000\n",
      "3   Bo    9000\n",
      "\n",
      "  emp  grp  salary\n",
      "0  Al  Fin    7000\n",
      "1  Ed  Eng    8000\n",
      "2  Jo  Eng   12000\n",
      "3  Bo   HR    9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df2=pd.DataFrame([['Al',7000],['Ed',8000],['Jo',12000],['Bo',9000]], columns=['name','salary'])\n",
    "res=pd.merge(df1, df2, left_on='emp', right_on='name').drop('name', axis=1)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a05d35",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.8 Merging on DataFrame Indices\n",
    "\n",
    "- Joining on the index in the left and right `DataFrames` with `merge()` and `left_index=True`, `right_index=True`.\n",
    "  - Here the merge is performed using the `Index` in the two `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0291c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     grp\n",
      "emp     \n",
      "Al   Fin\n",
      "Ed   Eng\n",
      "Jo   Eng\n",
      "Bo    HR\n",
      "\n",
      "     date\n",
      "emp      \n",
      "Jo   2004\n",
      "Al   2008\n",
      "Ed   2012\n",
      "Bo   2014\n",
      "\n",
      "     grp  date\n",
      "emp           \n",
      "Al   Fin  2008\n",
      "Ed   Eng  2012\n",
      "Jo   Eng  2004\n",
      "Bo    HR  2014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df1.set_index('emp', inplace=True)\n",
    "\n",
    "df2=pd.DataFrame([['Jo',2004],['Al',2008],['Ed',2012],['Bo',2014]], columns=['emp','date'])\n",
    "df2.set_index('emp', inplace=True)\n",
    "\n",
    "res=pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef367b",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.9 Merging on an Index and a Column\n",
    "\n",
    "- Joining on an index in one `DataFrame` and on a column in the other.\n",
    "  - Here the merge is performed using the `Index` in the first (left) `DataFrame` and the column `name` in the second (right) `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e56bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     grp\n",
      "emp     \n",
      "Al   Fin\n",
      "Ed   Eng\n",
      "Jo   Eng\n",
      "Bo    HR\n",
      "\n",
      "  name  salary\n",
      "0   Al    7000\n",
      "1   Ed    8000\n",
      "2   Jo   12000\n",
      "3   Bo    9000\n",
      "\n",
      "   grp name  salary\n",
      "0  Fin   Al    7000\n",
      "1  Eng   Ed    8000\n",
      "2  Eng   Jo   12000\n",
      "3   HR   Bo    9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al','Fin'],['Ed','Eng'],['Jo','Eng'],['Bo','HR']], columns=['emp','grp'])\n",
    "df1.set_index('emp', inplace=True)\n",
    "\n",
    "df2=pd.DataFrame([['Al',7000],['Ed',8000],['Jo',12000],['Bo',9000]], columns=['name','salary'])\n",
    "\n",
    "res=pd.merge(df1, df2, left_index=True, right_on='name')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10463eff",
   "metadata": {},
   "source": [
    "- If we want to rename column `name` to `emp` we can use the `rename()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c34198a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grp emp  salary\n",
      "0  Fin  Al    7000\n",
      "1  Eng  Ed    8000\n",
      "2  Eng  Jo   12000\n",
      "3   HR  Bo    9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res.rename(columns={'name': 'emp'}, inplace=True)\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339d5b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.10 Merging with Inner Join\n",
    "\n",
    "- Performing an inner join of two `DataFrames` with `merge()` and `how='inner'` (**default**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c3aa97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name   food\n",
      "0   Ed   fish\n",
      "1   Al  beans\n",
      "2   Bo  bread\n",
      "\n",
      "  name drink\n",
      "0   Bo  wine\n",
      "1   Jo  beer\n",
      "\n",
      "  name   food drink\n",
      "0   Bo  bread  wine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Ed','fish'],['Al','beans'],['Bo','bread']], columns=['name','food'])\n",
    "df2=pd.DataFrame([['Bo','wine'],['Jo','beer']], columns=['name','drink'])\n",
    "res=pd.merge(df1, df2)\n",
    "# res=pd.merge(df1, df2, how='inner')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a2aef",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.11 Merging with Outer Join\n",
    "\n",
    "- Performing an outer join of two `DataFrames` with `merge()` and `how='outer'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df428046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name   food\n",
      "0   Ed   fish\n",
      "1   Al  beans\n",
      "2   Bo  bread\n",
      "\n",
      "  name drink\n",
      "0   Bo  wine\n",
      "1   Jo  beer\n",
      "\n",
      "  name   food drink\n",
      "0   Al  beans   NaN\n",
      "1   Bo  bread  wine\n",
      "2   Ed   fish   NaN\n",
      "3   Jo    NaN  beer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Ed','fish'],['Al','beans'],['Bo','bread']], columns=['name','food'])\n",
    "df2=pd.DataFrame([['Bo','wine'],['Jo','beer']], columns=['name','drink'])\n",
    "res=pd.merge(df1, df2, how='outer')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539eb80",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.12 Merging with Left Join\n",
    "\n",
    "- Performing a left join of two `DataFrames` with `merge()` and `how='left'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b914b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name   food\n",
      "0   Ed   fish\n",
      "1   Al  beans\n",
      "2   Bo  bread\n",
      "\n",
      "  name drink\n",
      "0   Bo  wine\n",
      "1   Jo  beer\n",
      "\n",
      "  name   food drink\n",
      "0   Ed   fish   NaN\n",
      "1   Al  beans   NaN\n",
      "2   Bo  bread  wine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Ed','fish'],['Al','beans'],['Bo','bread']], columns=['name','food'])\n",
    "df2=pd.DataFrame([['Bo','wine'],['Jo','beer']], columns=['name','drink'])\n",
    "res=pd.merge(df1, df2, how='left')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf496a",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.13 Merging with Right Join\n",
    "\n",
    "- Performing a right join of two `DataFrames` with `merge()` and `how='right'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc27437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name   food\n",
      "0   Ed   fish\n",
      "1   Al  beans\n",
      "2   Bo  bread\n",
      "\n",
      "  name drink\n",
      "0   Bo  wine\n",
      "1   Jo  beer\n",
      "\n",
      "  name   food drink\n",
      "0   Bo  bread  wine\n",
      "1   Jo    NaN  beer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Ed','fish'],['Al','beans'],['Bo','bread']], columns=['name','food'])\n",
    "df2=pd.DataFrame([['Bo','wine'],['Jo','beer']], columns=['name','drink'])\n",
    "res=pd.merge(df1, df2, how='right')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ab0bd",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.13 Merging DataFrames with Common Non-Key Colums\n",
    "\n",
    "- Joining two DataFrames with `merge()` with non-key column names in common adds a default suffix.\n",
    "  - Here `rank` is such a column in the two `DataFrames`.\n",
    "  - Notice the resulting columns `rank_x` and `rank_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c69c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  rank\n",
      "0   Al     1\n",
      "1   Ed     2\n",
      "2   Bo     3\n",
      "3   Jo     4\n",
      "\n",
      "  name  rank\n",
      "0   Al     3\n",
      "1   Ed     1\n",
      "2   Bo     4\n",
      "3   Jo     2\n",
      "\n",
      "  name  rank_x  rank_y\n",
      "0   Al       1       3\n",
      "1   Ed       2       1\n",
      "2   Bo       3       4\n",
      "3   Jo       4       2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al',1],['Ed',2],['Bo',3],['Jo',4]], columns=['name','rank'])\n",
    "df2=pd.DataFrame([['Al',3],['Ed',1],['Bo',4],['Jo',2]], columns=['name','rank'])\n",
    "res=pd.merge(df1, df2, on='name')\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfdc16",
   "metadata": {},
   "source": [
    "- Choosing a suffix when joining two `DataFrames` with `merge()` and `suffixes=['left_suffix', 'right_suffix']`.\n",
    "  - Here we are using suffixes `_L` and `_R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c414726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  rank\n",
      "0   Al     1\n",
      "1   Ed     2\n",
      "2   Bo     3\n",
      "3   Jo     4\n",
      "\n",
      "  name  rank\n",
      "0   Al     3\n",
      "1   Ed     1\n",
      "2   Bo     4\n",
      "3   Jo     2\n",
      "\n",
      "  name  rank_L  rank_R\n",
      "0   Al       1       3\n",
      "1   Ed       2       1\n",
      "2   Bo       3       4\n",
      "3   Jo       4       2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame([['Al',1],['Ed',2],['Bo',3],['Jo',4]], columns=['name','rank'])\n",
    "df2=pd.DataFrame([['Al',3],['Ed',1],['Bo',4],['Jo',2]], columns=['name','rank'])\n",
    "res=pd.merge(df1, df2, on='name', suffixes=['_L','_R'])\n",
    "\n",
    "print(f'{df1}\\n')\n",
    "print(f'{df2}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2e99f",
   "metadata": {},
   "source": [
    "- If we want to rename the columns to `rank1` and `rank2` we can use the `rename()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79827193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  rank1  rank2\n",
      "0   Al      1      3\n",
      "1   Ed      2      1\n",
      "2   Bo      3      4\n",
      "3   Jo      4      2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.rename(columns={'rank_L':'rank1', 'rank_R':'rank2'}, inplace=True)\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7f538",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. Aggregation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80a268",
   "metadata": {},
   "source": [
    "## 11.1 Aggregation Methods\n",
    "\n",
    "- Pandas supports various aggregation methods.\n",
    "- They can be called on `Series` and `DataFrame`s.\n",
    "\n",
    "<img src=\"../images/pandas-aggregations.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59878887",
   "metadata": {},
   "source": [
    "---\n",
    "## 11.2 Aggregating Along an Axis\n",
    "\n",
    "- Aggregation methods, such as `mean()`, can be called on a `DataFrame`, specifying the axis of aggregation with `axis`.\n",
    "  - `axis=0` or `axis='rows'` (**default**) aggregates the values in the rows, yielding a result for each column.\n",
    "  - `axis=1` or `axis='columns'` aggregates the values in the columns, yielding a result for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a256b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  0.374540  0.155995\n",
      "1  0.950714  0.058084\n",
      "2  0.731994  0.866176\n",
      "3  0.598658  0.601115\n",
      "4  0.156019  0.708073\n",
      "\n",
      "A    0.562385\n",
      "B    0.477888\n",
      "dtype: float64\n",
      "\n",
      "0    0.265267\n",
      "1    0.504399\n",
      "2    0.799085\n",
      "3    0.599887\n",
      "4    0.432046\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "df = pd.DataFrame({'A': rng.rand(5), 'B': rng.rand(5)})\n",
    "res_rows = df.mean(axis='rows')    # same as axis=0\n",
    "res_cols = df.mean(axis='columns') # same as axis=1\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res_rows}\\n')\n",
    "print(f'{res_cols}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ccd942",
   "metadata": {},
   "source": [
    "---\n",
    "## 11.3 Summary Statistics with the `describe` Method\n",
    "\n",
    "- The method `describe()` can be called on a `DataFrame` which produces summary statistics for each numeric column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6a598139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  0.374540  0.155995\n",
      "1  0.950714  0.058084\n",
      "2  0.731994  0.866176\n",
      "3  0.598658  0.601115\n",
      "4  0.156019  0.708073\n",
      "\n",
      "              A         B\n",
      "count  5.000000  5.000000\n",
      "mean   0.562385  0.477888\n",
      "std    0.308748  0.353125\n",
      "min    0.156019  0.058084\n",
      "25%    0.374540  0.155995\n",
      "50%    0.598658  0.601115\n",
      "75%    0.731994  0.708073\n",
      "max    0.950714  0.866176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "df = pd.DataFrame({'A': rng.rand(5), 'B': rng.rand(5)})\n",
    "res = df.describe()\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbe835",
   "metadata": {},
   "source": [
    "---\n",
    "# 12. Grouping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbc54b",
   "metadata": {},
   "source": [
    "## 12.1 The Split-Apply-Merge Scheme\n",
    "\n",
    "- Pandas supports grouping operations on `DataFrame`s.\n",
    "- This is done by calling the `groupby()` method.\n",
    "  - It is equivalent to a `GROUP BY` in SQL.\n",
    "  - It creates a `DataFrameGroupby` object.\n",
    "    - Aggregation methods are then called on it.\n",
    "- The workflow follows a **split-apply-merge** scheme.\n",
    "  - A `groupby()` **splits** the `DataFrame` into groups.\n",
    "  - Then aggregation methods are **applied** to the groups.\n",
    "  - Finally, the aggregation results are **combined** back into a `DataFrame`.\n",
    "  - E.g.: `df2 = df1.groupby('col').sum()`\n",
    "\n",
    "<img src=\"../images/splt-apply-merge.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee60b2",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.2 Grouping on a Column and Calculating the Sum\n",
    "\n",
    "- Grouping on the column `'key'` and computing the `sum()` of each column `'data1'` and `'data2'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f612e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "     data1  data2\n",
      "key              \n",
      "A        3      8\n",
      "B        5      7\n",
      "C        7     12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res = df.groupby('key').sum()\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92977e32",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.3 Grouping on a Column and Calculating the Median\n",
    "\n",
    "- Grouping on the column `'key'`, selecting grouped column `'data1'`, and computing the `median()` for column `'data1'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "80acdc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "key\n",
      "A    1.5\n",
      "B    2.5\n",
      "C    3.5\n",
      "Name: data1, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res = df.groupby('key')['data1'].median()\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f258e8c",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.4 Grouping and Using the `aggregate` Method\n",
    "\n",
    "- Grouping on the column `'key'`, and using the `aggregate()` method.\n",
    "  - The `aggregate()` method accepts a list of aggregation methods (or their names) that are, each, applied to the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2c5c6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "    data1            data2           \n",
      "      min median max   min median max\n",
      "key                                  \n",
      "A       0    1.5   3     3    4.0   5\n",
      "B       1    2.5   4     0    3.5   7\n",
      "C       2    3.5   5     3    6.0   9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res = df.groupby('key').aggregate(['min', np.median, max])\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f2ff5",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.5 Grouping and Using the `filter` Method\n",
    "\n",
    "- Grouping on the column `'key'`, and using the `filter()` method.\n",
    "  - The `filter()` method accepts a function (or lambda) with the grouped `DataFrame` as an argument (here `x`).\n",
    "  - It then filters the groups according to some criteria (here the standard deviation in column `'data2' > 4`).\n",
    "  - Only groups that meet the criteria are kept (here groups `'B'` and `'C'` since the standard deviation in group `'A' <= 4`).\n",
    "  - It returns the same amount of columns as in the original `DataFrame` (it just filters out rows, i.e. it doesn’t return grouped data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c77dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "       data1     data2\n",
      "key                   \n",
      "A    2.12132  1.414214\n",
      "B    2.12132  4.949747\n",
      "C    2.12132  4.242641\n",
      "\n",
      "  key  data1  data2\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res1 = df.groupby('key').std()\n",
    "\n",
    "res2 = df.groupby('key').filter(lambda x: x['data2'].std() > 4)\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res1}\\n')\n",
    "print(f'{res2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563894c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.6 Grouping and Using the `transform` Method\n",
    "\n",
    "- Grouping on the column `'key'`, and using the `transform()` method.\n",
    "  - The `transform()` method accepts a function (or lambda) with the grouped `DataFrame` as an argument (here `x`).\n",
    "  - It transforms each row according to some criteria (here the mean of each column is subtracted from each of its elements `x - x.mean()`).\n",
    "  - It returns the same amount of rows as in the original `DataFrame` (it just transforms the data, i.e. it doesn’t return grouped data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eef5dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "   data1  data2\n",
      "0   -1.5    1.0\n",
      "1   -1.5   -3.5\n",
      "2   -1.5   -3.0\n",
      "3    1.5   -1.0\n",
      "4    1.5    3.5\n",
      "5    1.5    3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res = df.groupby('key').transform(lambda x: x - x.mean())\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e79cb",
   "metadata": {},
   "source": [
    "---\n",
    "## 12.7 Grouping and Using the `apply` Method\n",
    "\n",
    "- Grouping on the column `'key'`, and using the `apply()` method.\n",
    "  - The `apply()` method accepts a function (or lambda) with the grouped `DataFrame` as an argument (here `x`).\n",
    "  - It applies the function to each row according to some criteria (here `'data1'` is divided by the sum in `'data2'` and the result stored in `'data1'`).\n",
    "  - It returns the same amount of rows as in the original `DataFrame` (it just applies the function to the data, i.e. it doesn’t return grouped data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f3ce730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  data1  data2\n",
      "0   A      0      5\n",
      "1   B      1      0\n",
      "2   C      2      3\n",
      "3   A      3      3\n",
      "4   B      4      7\n",
      "5   C      5      9\n",
      "\n",
      "      data1  data2\n",
      "0  0.000000      5\n",
      "1  0.375000      3\n",
      "2  0.142857      0\n",
      "3  0.571429      7\n",
      "4  0.166667      3\n",
      "5  0.416667      9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def norm_by_data2(x):\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "df = pd.DataFrame({'key': ['A','B','C','A','B','C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0,10,6)})\n",
    "\n",
    "res = df.groupby('key')[['data1','data2']].apply(norm_by_data2).reset_index(drop=True)\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f725d8",
   "metadata": {},
   "source": [
    "---\n",
    "# 13. Vectorized String Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556b644",
   "metadata": {},
   "source": [
    "## 13.1 Using the `str` attribute with String Methods\n",
    "\n",
    "- Pandas provides Python’s string manipulation methods via the attribute `str`.\n",
    "- It is used on `Series` (or `DataFrame` columns) that contain strings, e.g.:\n",
    "  - Capitalize each string in a `Series`\n",
    "\n",
    "    ```python\n",
    "    res = s.str.capitalize()\n",
    "    ```\n",
    "  \n",
    "  - Capitalize each string in a `DataFrame` column\n",
    "\n",
    "    ```python\n",
    "    res = df['col'].str.capitalize()\n",
    "    ```\n",
    "\n",
    "- The example below uses the `str` attribute in a `Series` to capitalize each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5ee1537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Paul\n",
      "1     None\n",
      "2     MARY\n",
      "3    gUIDO\n",
      "dtype: object\n",
      "\n",
      "0     Paul\n",
      "1     None\n",
      "2     Mary\n",
      "3    Guido\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = pd.Series(['Paul', None, 'MARY', 'gUIDO'])\n",
    "res = s.str.capitalize()\n",
    "\n",
    "print(f'{s}\\n')\n",
    "print(f'{res}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3133ba",
   "metadata": {},
   "source": [
    "---\n",
    "# 14. Working with Time Series\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafb54f",
   "metadata": {},
   "source": [
    "## 14.1 Date and Time Data Types in Pandas\n",
    "\n",
    "- Pandas combines Numpy’s `datetime64` datatype with Python’s date and time functions for vectorized date and time functionality in `Series` and `DataFrame`s.\n",
    "- The datatypes used for date and time in Pandas are:\n",
    "  - `Timestamp` which represents a date and time value (e.g. 09:32:50 on the 4 th of July 2015).\n",
    "  - `DatetimeIndex` which represents a date and time index (used to index each data point in a time series).\n",
    "  - `Period` which represents a date and time period (e.g. an hourly, daily, weekly, monthly, yearly, etc.).\n",
    "  - `PeriodIndex` which represents a period index.\n",
    "  - `Timedelta` which represents a date and time difference (e.g. 1 hour, 2 days, 5 weeks, 1 month, 1 year, etc)\n",
    "  - `TimedeltaIndex` which represents a timedelta index.\n",
    "- The methods and functions used for time series in Pandas are:\n",
    "  - `pd.to_datetime()`, `pd.to_period()`, `pd.to_timedelta()` which convert dates/times to Pandas types.\n",
    "  - `pd.date_range()`, `pd.period_range()`, `pd.timedelta_range()` which create ranges.\n",
    "  - `resample()` and `asfreq()` which resample time series, e.g. converting a daily to hourly time series.\n",
    "  - `shift()` which shifts the elements in a time series forward or backward a number of steps.\n",
    "  - `rolling()` which creates a rolling window from a time series, used to compute aggregates (e.g. rolling mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78e0b2",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.2 Dates and Times in Python\n",
    "\n",
    "- Dates and times are created in Python using the built-in type `datetime`.\n",
    "- Useful methods are also available via the `parser` class in the external package `dateutil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "118a8819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-04 00:00:00\n",
      "2015-07-04 00:00:00\n",
      "Saturday\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "date1 = datetime(year=2015, month=7, day=4) # create a Python datetime\n",
    "\n",
    "from dateutil import parser\n",
    "date2 = parser.parse('4th of July, 2015') # parse a date and time string\n",
    "\n",
    "res = date2.strftime('%A') # format datetime as a string with day of week\n",
    "\n",
    "print(date1)\n",
    "print(date2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd0f96",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.3 Dates and Times in Numpy\n",
    "\n",
    "- Numpy’s `datetime64` datatype is a high-resolution date and time datatype.\n",
    "- It can be passed a code to determine the resolution (see table).\n",
    "  - E.g. `'ns'` for a nanosecond resolution.\n",
    "\n",
    "<img src=\"../images/np-datetime-codes.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f445905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-04\n",
      "2015-07-05\n",
      "['2015-07-04' '2015-07-05' '2015-07-06']\n",
      "1 days\n",
      "2015-07-04T12:59:59.500000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "date1 = np.array('2015-07-04', dtype=np.datetime64)\n",
    "date2 = np.array('2015-07-05', dtype=np.datetime64)\n",
    "date3 = date1 + np.arange(3)\n",
    "date4 = date2 - date1\n",
    "date5 = np.datetime64('2015-07-04 12:59:59.50', 'ns')\n",
    "\n",
    "print(date1)\n",
    "print(date2)\n",
    "print(date3)\n",
    "print(date4)\n",
    "print(date5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499defd",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.4 Dates and Times in Pandas\n",
    "\n",
    "- Pandas combines Numpy’s `datetime64` (resolution) with Python’s `datetime` (methods).\n",
    "  - `pd.to_datetime()` converts strings to Pandas `Timestamps`.\n",
    "  - `strftime()` formats a `Timestamp` into a string (e.g. with the day of week).\n",
    "  - `to_timedelta()` creates a time difference (e.g. an array of a 1-day and a 2-day `Timedelta`).\n",
    "  - A code can be used to determine the resolution of Pandas date and time datatypes (see table).\n",
    "\n",
    "<img src=\"../images/pandas-datetime-codes.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e302b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-04 00:00:00\n",
      "2015-07-05 00:00:00\n",
      "Sunday\n",
      "DatetimeIndex(['2015-07-05', '2015-07-04'], dtype='datetime64[ns]', freq=None)\n",
      "1 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "date1 = pd.to_datetime('2015-07-04')\n",
    "date2 = pd.to_datetime('5th of July, 2015')\n",
    "date3 = date2.strftime('%A')\n",
    "date4 = date2 - pd.to_timedelta(np.arange(2), 'D')\n",
    "date5 = date2 - date1\n",
    "\n",
    "print(date1)\n",
    "print(date2)\n",
    "print(date3)\n",
    "print(date4)\n",
    "print(date5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621a2f4",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.5 DatetimeIndex and Indexing\n",
    "\n",
    "- A `DatetimeIndex` is used as a date and time `index` in `Series` and `DataFrames`.\n",
    "- Indexing is done similarly to Python list indexing and Numpy `ndarray` indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf735c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-07-04    0\n",
      "2014-08-04    1\n",
      "2015-07-04    2\n",
      "2015-08-04    3\n",
      "dtype: int64\n",
      "\n",
      "2014-07-04    0\n",
      "2014-08-04    1\n",
      "2015-07-04    2\n",
      "dtype: int64\n",
      "\n",
      "2015-07-04    2\n",
      "2015-08-04    3\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04','2014-08-04','2015-07-04','2015-08-04']) # index\n",
    "\n",
    "data = pd.Series([0,1,2,3], index=index) # use DatetimeIndex as index in Series\n",
    "data1 = data['2014-07-04':'2015-07-04']  # range indexing between start to stop datetime\n",
    "data2 = data['2015']                     # indexing one datetime (here just a year)\n",
    "\n",
    "print(f'{data}\\n')\n",
    "print(f'{data1}\\n')\n",
    "print(f'{data2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3e2d3",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.6 to_datetime(), to_period(), and to_timedelta()\n",
    "- The Pandas datatypes `Timestamp` and `DatetimeIndex`, `Period` and `PeriodIndex`, `Timedelta` and `TimedeltaIndex`\n",
    "  - `pd.to_datetime()` creates a `Timestamp` from a single datetime string, but a `DatetimeIndex` from a list of datetime strings.\n",
    "  - `pd.to_period()` creates a `Period` from a `Timestamp` object, but a `PeriodIndex` from a `DatetimeIndex` object.\n",
    "  - `pd.to_timedelta()` creates a `Timedelta` from a `Timestamp` object, but a `TimedeltaIndex` from a `DatetimeIndex` object.\n",
    "  - Subtracting two `Timestamps` creates a `Timedelta`, whereas subtractions involving a `DatetimeIndex` creates a `TimedeltaIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57815231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-03 00:00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "DatetimeIndex(['2015-07-03', '2015-07-04'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "2015-07-03\n",
      "<class 'pandas._libs.tslibs.period.Period'>\n",
      "PeriodIndex(['2015-07-03', '2015-07-04'], dtype='period[D]')\n",
      "\n",
      "1 days 00:00:00\n",
      "<class 'pandas._libs.tslibs.timedeltas.Timedelta'>\n",
      "TimedeltaIndex(['0 days', '1 days'], dtype='timedelta64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "date = pd.to_datetime('2015-07-03')\n",
    "dates = pd.to_datetime(['2015-07-03','2015-07-04'])\n",
    "\n",
    "period = date.to_period('D')\n",
    "periods = dates.to_period('D')\n",
    "\n",
    "timedelta = dates[1] - dates[0]\n",
    "timedeltas = dates - date\n",
    "\n",
    "print(date)\n",
    "print(type(date))\n",
    "print(dates)\n",
    "\n",
    "print()\n",
    "\n",
    "print(period)\n",
    "print(type(period))\n",
    "print(periods)\n",
    "\n",
    "print()\n",
    "\n",
    "print(timedelta)\n",
    "print(type(timedelta))\n",
    "print(timedeltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd189a9",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.7 date_range(), period_range(), and timedelta_range()\n",
    "\n",
    "- We can create a range of datetimes using the `pd.date_range()` function:\n",
    "  - Between a start `'2015-07-03'` and end datetime `'2015-07-05'`.\n",
    "  - From a start `'2015-07-03'` datetime and the number of `periods=3`.\n",
    "  - From a start `'2015-07-03'` datetime and the number of `periods=3`, with a specific frequency (resolution), e.g. hourly `freq='h'`.\n",
    "- We can also create a range of periods and timedeltas (with periods and frequencies) using the functions `pd.period_range()` and `pd.timedelta_range()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b60e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05'], dtype='datetime64[ns]', freq='D')\n",
      "\n",
      "DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05'], dtype='datetime64[ns]', freq='D')\n",
      "\n",
      "DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00',\n",
      "               '2015-07-03 02:00:00'],\n",
      "              dtype='datetime64[ns]', freq='h')\n",
      "\n",
      "PeriodIndex(['2015-07', '2015-08', '2015-09'], dtype='period[M]')\n",
      "\n",
      "TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00'], dtype='timedelta64[ns]', freq='h')\n"
     ]
    }
   ],
   "source": [
    "date1 = pd.date_range('2015-07-03','2015-07-05')\n",
    "date2 = pd.date_range('2015-07-03', periods=3)\n",
    "date3 = pd.date_range('2015-07-03', periods=3, freq='h')\n",
    "period = pd.period_range('2015-07', periods=3, freq='M')\n",
    "timedelta = pd.timedelta_range(0, periods=3, freq='h')\n",
    "\n",
    "print(date1)\n",
    "print()\n",
    "\n",
    "print(date2)\n",
    "print()\n",
    "\n",
    "print(date3)\n",
    "print()\n",
    "\n",
    "print(period)\n",
    "print()\n",
    "\n",
    "print(timedelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf8cb6",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.8 Resampling a Time Series\n",
    "\n",
    "- A `Series` or `DataFrame` can be resampled using the methods `resample()` or `asfreq()`.\n",
    "- `resample()` takes a resolution code `'A'` argument to determine the resolution, and requires an aggregate method to be called, e.g. `mean()`.\n",
    "  - It calculates the aggregate (using the aggregation method) and uses this as the value of the final data point for each period.\n",
    "  - Notice the values below are averages (due to the `mean()` aggregate method) of a year-end resolution (due to the code `'A'`)\n",
    "- `asfreq()` takes a resolution code `'A'` argument to determine the resolution, and returns the time series with the new resolution.\n",
    "  - It doesn’t calculate any aggregate for the the final data point for each period.\n",
    "  - Notice the values below are the year-end values (due to the code `'A'`), and there is no year-end value for `'2005'`, only for `'2004'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2053fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-23    54.65\n",
      "2004-08-24    52.38\n",
      "2004-08-25    52.95\n",
      "2004-12-31    52.37\n",
      "2005-01-01    52.38\n",
      "Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-12-31    52.761667\n",
      "2005-12-31    52.380000\n",
      "Freq: YE-DEC, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-12-31    52.37\n",
      "Freq: YE-DEC, Name: Close, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "index = pd.DatetimeIndex(['2004-08-19','2004-08-20','2004-08-23','2004-08-24','2004-08-25','2004-12-31','2005-01-01'], name='Date')\n",
    "s = pd.Series([50.12, 54.10, 54.65, 52.38, 52.95, 52.37, 52.38], index=index, name='Close')\n",
    "res1 = s.resample('A').mean() # A=Year End\n",
    "res2 = s.asfreq('A') # A=Year End\n",
    "\n",
    "print(f'{s}\\n')\n",
    "print(f'{res1}\\n')\n",
    "print(f'{res2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f857ccb",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.9 Shifting a Time Series (by Value)\n",
    "\n",
    "- The `shift(time_steps)` method shifts a time series forward (positive) or backward (negative) a number of `time_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d775eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-23    54.65\n",
      "2004-08-24    52.38\n",
      "2004-08-25    52.95\n",
      "2004-12-31    52.37\n",
      "2005-01-01    52.38\n",
      "Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-21      NaN\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19      NaN\n",
      "2004-08-20    50.12\n",
      "2004-08-21    54.10\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19    54.1\n",
      "2004-08-20     NaN\n",
      "2004-08-21     NaN\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=pd.DatetimeIndex(['2004-08-19','2004-08-20','2004-08-23','2004-08-24','2004-08-25','2004-12-31','2005-01-01'], name='Date')\n",
    "s = pd.Series([50.12, 54.10, 54.65, 52.38, 52.95, 52.37, 52.38], index=index, name='Close')\n",
    "\n",
    "res1 = s.asfreq('D').head(3)           # head(3) shows the first three rows in the DataFrame\n",
    "res2 = s.asfreq('D').shift(1).head(3)  # shifts the values forward 1 step (the new first data point is NaN)\n",
    "res3 = s.asfreq('D').shift(-1).head(3) # shifts the values backward 1 step (the new last data point is NaN)\n",
    "\n",
    "print(f'{s}\\n')\n",
    "print(f'{res1}\\n')\n",
    "print(f'{res2}\\n')\n",
    "print(f'{res3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a46fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.10 Shifting a Time Series (by Index)\n",
    "\n",
    "- The `shift(time_steps)` method shifts a time series forward (positive) or backward (negative) a number of `time_steps`.\n",
    "  - Notice the use of the keyword `freq`in the `shift()` method to shift the `Index` instead of the `Value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a22fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-23    54.65\n",
      "2004-08-24    52.38\n",
      "2004-08-25    52.95\n",
      "2004-12-31    52.37\n",
      "2005-01-01    52.38\n",
      "Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-21      NaN\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-20    50.12\n",
      "2004-08-21    54.10\n",
      "2004-08-22      NaN\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-18    50.12\n",
      "2004-08-19    54.10\n",
      "2004-08-20      NaN\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=pd.DatetimeIndex(['2004-08-19','2004-08-20','2004-08-23','2004-08-24','2004-08-25','2004-12-31','2005-01-01'], name='Date')\n",
    "s = pd.Series([50.12, 54.10, 54.65, 52.38, 52.95, 52.37, 52.38], index=index, name='Close')\n",
    "\n",
    "res1 = s.asfreq('D').head(3)                     # head(3) shows the first three rows in the DataFrame\n",
    "res2 = s.asfreq('D').shift(1, freq='D').head(3)  # shifts the index backward 1 step\n",
    "res3 = s.asfreq('D').shift(-1, freq='D').head(3) # shifts the index forward 1 step\n",
    "\n",
    "print(f'{s}\\n')\n",
    "print(f'{res1}\\n')\n",
    "print(f'{res2}\\n')\n",
    "print(f'{res3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef190638",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.11 Rolling Aggregates\n",
    "\n",
    "- `rolling()` returns an object that, functionally, slides a window of a given size, e.g. `2`, over the time series.\n",
    "  - Then an aggregation method, e.g. `mean()`, can be called on it to calculate an aggregate value for the data points within each window.\n",
    "  - An optional parameter `center=True` can be passed to `rolling()` to center the data points within the time series’ original length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f34164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2004-08-19    50.12\n",
      "2004-08-20    54.10\n",
      "2004-08-21    54.10\n",
      "2004-08-22    54.10\n",
      "2004-08-23    54.65\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19       NaN\n",
      "2004-08-20    52.110\n",
      "2004-08-21    54.100\n",
      "2004-08-22    54.100\n",
      "2004-08-23    54.375\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n",
      "Date\n",
      "2004-08-19         NaN\n",
      "2004-08-20    2.814285\n",
      "2004-08-21    0.000000\n",
      "2004-08-22    0.000000\n",
      "2004-08-23    0.388909\n",
      "Freq: D, Name: Close, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=pd.DatetimeIndex(['2004-08-19','2004-08-20','2004-08-23','2004-08-24','2004-08-25','2004-12-31','2005-01-01'], name='Date')\n",
    "s = pd.Series([50.12, 54.10, 54.65, 52.38, 52.95, 52.37, 52.38], index=index, name='Close')\n",
    "s = s.asfreq('D').ffill()\n",
    "\n",
    "rolling = s.rolling(2, center=True)\n",
    "\n",
    "res1 = rolling.mean().head() # head() shows the first 5 rows\n",
    "res2 = rolling.std().head()  # head() shows the first 5 rows\n",
    "\n",
    "print(f'{s.head()}\\n')       # head() shows the first 5 rows\n",
    "print(f'{res1}\\n')\n",
    "print(f'{res2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ed70e",
   "metadata": {},
   "source": [
    "---\n",
    "# 15. Saving and Loading Series and DataFrames\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0a04c",
   "metadata": {},
   "source": [
    "## 15.1 CSV, Excel, JSON, and Pickle Files\n",
    "- A `Series` or `DataFrame` can be saved to disk using `to_XYZ`, where `XYZ` is a file format.\n",
    "  - `df.to_csv('filename.csv')` saves to a CSV (Comma Separated Values) file.\n",
    "  - `df.to_excel('filename.xlsx')` saves to an Excel file.\n",
    "  - `df.to_json('filename.json')` saves to a JSON (JavaScript Object Notation) file.\n",
    "  - `df.to_pickle('filename.pkl')` saves to a Pickle file (a compact binary format).\n",
    "- A `Series` or `DataFrame` can be loaded from disk using `pd.read_XYZ`, where `XYZ` is a file format.\n",
    "  - `df = pd.read_csv('filename.csv')` loads from a CSV file.\n",
    "  - `df = pd.read_excel('filename.xlsx')` loads from a Excel file.\n",
    "  - `df = pd.read_json('filename.json')` loads from a JSON file.\n",
    "  - `df = pd.read_pickle('filename.pkl')` loads from a Pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd53df",
   "metadata": {},
   "source": [
    "---\n",
    "## 15.2 Saving a DataFrame to a CSV, Excel, JSON, and a Pickle File\n",
    "- Let’s say we want to save the `DataFrame` below, and then load it back again, where we want to preserve all data with data types (`dtype`), index type, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a03c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeid  name  age   salary  fulltime   hiredate\n",
      "idx                                                     \n",
      "0             1  John   25  25000.0      True 2001-01-01\n",
      "1             2  Jane   22  22000.0     False 2002-02-02\n",
      "\n",
      "employeeid             int64\n",
      "name          string[python]\n",
      "age                    int32\n",
      "salary               float32\n",
      "fulltime                bool\n",
      "hiredate      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Index([0, 1], dtype='int32', name='idx')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = pd.Index([0,1], dtype='int32', name='idx')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'employeeid': pd.Series([1, 2], dtype='int64'),\n",
    "        'name': pd.Series(['John', 'Jane'], dtype='string'),\n",
    "        'age': pd.Series([25, 22], dtype='int32'),\n",
    "        'salary': pd.Series([25000, 22000], dtype='float32'),\n",
    "        'fulltime': pd.Series([True, False], dtype='bool'),\n",
    "        'hiredate': pd.Series(['2001-01-01', '2002-02-02'], dtype='datetime64[ns]')\n",
    "    },\n",
    "    index=index)\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{df.dtypes}\\n')\n",
    "print(f'{df.index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd026f51",
   "metadata": {},
   "source": [
    "- When we save the `DataFrame`, and want to include as much information as possible:\n",
    "  - `df.to_csv('employees.csv')` saves to a CSV file.\n",
    "  - `df.to_excel('employees.xlsx')` saves to an Excel file.\n",
    "  - `df.to_json('employees.json', orient='index')` saves to a JSON file.\n",
    "  - `df.to_pickle('employees.pkl')` saves to a Pickle file.\n",
    "- Notice we need `orient='index'` to include the index when saving to a JSON file.\n",
    "  - If we don’t want to save the index in the file, we omit `orient='index'`.\n",
    "- When saving to CSV or Excel, the index is included in the file by default.\n",
    "  - If we don’t want to save the index in the file, we can use `index=False`.\n",
    "- A Pickle file saves all information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f9bfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('employees.csv')\n",
    "df.to_excel('employees.xlsx')\n",
    "df.to_json('employees.json', orient='index')\n",
    "df.to_pickle('employees.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0250f7",
   "metadata": {},
   "source": [
    "---\n",
    "## 15.3 Loading a DataFrame from a CSV File\n",
    "\n",
    "- When we load the `DataFrame` from a CSV file, and want the original `DataFrame` fully recreated:\n",
    "  - We need to specify the data types for each column via the `dtype` parameter.\n",
    "  - We need to specify the columns of type `datetime64` separately via the `parse_dates` parameter.\n",
    "  - We need to explicitly set the index type we want after we have loaded the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96520842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeid  name  age   salary  fulltime   hiredate\n",
      "idx                                                     \n",
      "0             1  John   25  25000.0      True 2001-01-01\n",
      "1             2  Jane   22  22000.0     False 2002-02-02\n",
      "\n",
      "employeeid             int64\n",
      "name          string[python]\n",
      "age                    int32\n",
      "salary               float32\n",
      "fulltime                bool\n",
      "hiredate      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Index([0, 1], dtype='int32', name='idx')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = {'employeeid': 'int64',\n",
    "      'name': 'string',\n",
    "      'age': 'int32',\n",
    "      'salary': 'float32',\n",
    "      'fulltime': 'bool'}\n",
    "\n",
    "pds=['hiredate']\n",
    "\n",
    "df = pd.read_csv('employees.csv', dtype=dt, parse_dates=pds, index_col='idx')\n",
    "\n",
    "df.index = pd.Index(df.index, dtype='int32', name='idx')\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{df.dtypes}\\n')\n",
    "print(f'{df.index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801951e",
   "metadata": {},
   "source": [
    "---\n",
    "## 15.4 Loading a DataFrame from an Excel File\n",
    "\n",
    "- When we load the `DataFrame` from a XLSX file, and want the original `DataFrame` fully recreated:\n",
    "  - We need to specify the data types for each column via the `dtype` parameter.\n",
    "  - We need to specify the columns of type `datetime64` separately via the `parse_dates` parameter.\n",
    "  - We need to explicitly set the index type we want after we have loaded the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ded019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeid  name  age   salary  fulltime   hiredate\n",
      "idx                                                     \n",
      "0             1  John   25  25000.0      True 2001-01-01\n",
      "1             2  Jane   22  22000.0     False 2002-02-02\n",
      "\n",
      "employeeid             int64\n",
      "name          string[python]\n",
      "age                    int32\n",
      "salary               float32\n",
      "fulltime                bool\n",
      "hiredate      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Index([0, 1], dtype='int32', name='idx')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = {'employeeid': 'int64',\n",
    "      'name': 'string',\n",
    "      'age': 'int32',\n",
    "      'salary': 'float32',\n",
    "      'fulltime': 'bool'}\n",
    "\n",
    "pds=['hiredate']\n",
    "\n",
    "df = pd.read_excel('employees.xlsx', dtype=dt, parse_dates=pds, index_col='idx')\n",
    "\n",
    "df.index = pd.Index(df.index, dtype='int32', name='idx')\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{df.dtypes}\\n')\n",
    "print(f'{df.index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f075379",
   "metadata": {},
   "source": [
    "---\n",
    "## 15.5 Loading a DataFrame from a JSON File\n",
    "\n",
    "- When we load the `DataFrame` from a JSON file, and want the original `DataFrame` fully recreated:\n",
    "  - We need to specify the data types for each column using the `dtype` parameter.\n",
    "  - We need to specify `orient='index'` to include the index from the file.\n",
    "  - We need to explicitly set the index type we want after we have loaded the file.\n",
    "  - We need to explicitly convert columns of type `datetime64` with `pd.to_datetime()` after\n",
    "loading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c0c6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeid  name  age   salary  fulltime   hiredate\n",
      "idx                                                     \n",
      "0             1  John   25  25000.0      True 2001-01-01\n",
      "1             2  Jane   22  22000.0     False 2002-02-02\n",
      "\n",
      "employeeid             int64\n",
      "name          string[python]\n",
      "age                    int32\n",
      "salary               float32\n",
      "fulltime                bool\n",
      "hiredate      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Index([0, 1], dtype='int32', name='idx')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = {'employeeid': 'int64',\n",
    "      'name': 'string',\n",
    "      'age': 'int32',\n",
    "      'salary': 'float32',\n",
    "      'fulltime': 'bool'}\n",
    "\n",
    "df = pd.read_json('employees.json', dtype=dt, orient='index')\n",
    "\n",
    "df.index = pd.Index(df.index, dtype='int32', name='idx')\n",
    "\n",
    "df['hiredate'] = pd.to_datetime(df['hiredate'], unit='ms')\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{df.dtypes}\\n')\n",
    "print(f'{df.index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1fa5a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 15.6 Loading a DataFrame from a Pickle File\n",
    "\n",
    "- When we load the `DataFrame` from a PKL file, and want the original `DataFrame` fully recreated:\n",
    "  - We just have to load the pickle file since all information is already included in the file.\n",
    "  - So, to preserve all information when saving and loading a `DataFrame` or `Series`, use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8565733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeid  name  age   salary  fulltime   hiredate\n",
      "idx                                                     \n",
      "0             1  John   25  25000.0      True 2001-01-01\n",
      "1             2  Jane   22  22000.0     False 2002-02-02\n",
      "\n",
      "employeeid             int64\n",
      "name          string[python]\n",
      "age                    int32\n",
      "salary               float32\n",
      "fulltime                bool\n",
      "hiredate      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Index([0, 1], dtype='int32', name='idx')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('employees.pkl')\n",
    "\n",
    "print(f'{df}\\n')\n",
    "print(f'{df.dtypes}\\n')\n",
    "print(f'{df.index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b99842",
   "metadata": {},
   "source": [
    "---\n",
    "# 16. Cleanup\n",
    "---\n",
    "\n",
    "- Let's remove all files that have been created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60dae0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "dirs = []\n",
    "files = ['employees.csv', 'employees.xlsx', 'employees.json', 'employees.pkl']\n",
    "\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878680b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "fsharp",
    "items": [
     {
      "aliases": [],
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
